# =============================================================================
# Centralized Model Configuration for D-CIPHER
# =============================================================================
# This file defines all supported models with their backend, pricing, and limits.
# Edit this file to add new models or update pricing - no Python code changes needed.
#
# Structure for each model:
#   "model-name":
#     backend: <backend_name>          # openai, anthropic, gemini, vertexai, ollama, openrouter, together
#     max_context: <int>               # Maximum context window in tokens
#     cost_per_input_token: <float>    # Cost per input token in dollars
#     cost_per_output_token: <float>   # Cost per output token in dollars
# =============================================================================

# -----------------------------------------------------------------------------
# Vertex AI Gemini Models

"gemini-3-pro-preview":
  backend: vertexai
  max_context: 128000
  cost_per_input_token: 2e-06
  cost_per_output_token: 1.2e-05

"gemini-3-flash-preview":
  backend: vertexai
  max_context: 1000000
  cost_per_input_token: 5e-07
  cost_per_output_token: 3e-06

"gemini-2.5-pro":
  backend: vertexai
  max_context: 2000000
  cost_per_input_token: 1.25e-06
  cost_per_output_token: 1e-05

"gemini-2.5-flash":
  backend: vertexai
  max_context: 1000000
  cost_per_input_token: 3e-07
  cost_per_output_token: 2.5e-06

# -----------------------------------------------------------------------------
# Vertex AI Partner Models (Llama, Qwen, etc.)

"llama-3.3-70b-instruct":
  backend: vertexai
  # backend: vertexai_endpoint
  # endpoint_url: "https://mg-endpoint-de5d6bfe-3f12-455a-a4bc-1c72421cd592.us-central1-473120694576.prediction.vertexai.goog/v1beta1"
  max_context: 128000
  cost_per_input_token: 8.8e-07
  cost_per_output_token: 8.8e-07

"llama-3.1-8b-instruct":
  backend: vertexai
  max_context: 128000
  cost_per_input_token: 2e-07
  cost_per_output_token: 2e-07

"Qwen3-30B-A3B-Instruct":
  backend: vertexai
  max_context: 32768
  cost_per_input_token: 5e-07
  cost_per_output_token: 5e-07

"fdtn-ai/Foundation-Sec-1.1-8B-Instruct":
  backend: vertexai
  max_context: 8192
  cost_per_input_token: 2e-07
  cost_per_output_token: 2e-07

# -----------------------------------------------------------------------------
# Ollama Models

"llama3.2:1b":
  backend: ollama
  max_context: 131072
  cost_per_input_token: 0
  cost_per_output_token: 0

# -----------------------------------------------------------------------------
# Anthropic via OpenRouter

"anthropic/claude-opus-4.5":
  backend: openrouter
  max_context: 200000
  cost_per_input_token: 5e-06
  cost_per_output_token: 2.5e-05

"anthropic/claude-sonnet-4.5":
  backend: openrouter
  max_context: 200000
  cost_per_input_token: 3e-06
  cost_per_output_token: 15e-06

"anthropic/claude-haiku-4.5":
  backend: openrouter
  max_context: 200000
  cost_per_input_token: 1e-06
  cost_per_output_token: 5e-06

# OpenAI via OpenRouter
"openai/gpt-5.2":
  backend: openrouter
  max_context: 400000
  cost_per_input_token: 1.75e-06
  cost_per_output_token: 1.4e-05

"openai/gpt-5.2-pro":
  backend: openrouter
  max_context: 400000
  cost_per_input_token: 2.1e-05
  cost_per_output_token: 1.68e-04

"openai/gpt-5.2-codex":
  backend: openrouter
  max_context: 128000
  cost_per_input_token: 1.75e-06
  cost_per_output_token: 1.4e-05

# DeepSeek via OpenRouter
"deepseek/deepseek-r1-0528":
  backend: openrouter
  max_context: 163840
  cost_per_input_token: 4e-07
  cost_per_output_token: 1.75e-06

"deepseek/deepseek-v3.2":
  backend: openrouter
  max_context: 163840
  cost_per_input_token: 2.6e-07
  cost_per_output_token: 3.8e-07

# xAI Grok via OpenRouter
"x-ai/grok-4":
  backend: openrouter
  max_context: 131072
  cost_per_input_token: 3e-06
  cost_per_output_token: 1.5e-05

"x-ai/grok-4.1-fast":
  backend: openrouter
  max_context: 131072
  cost_per_input_token: 5e-07
  cost_per_output_token: 2.5e-06

# Mistral via OpenRouter
"mistralai/mistral-large":
  backend: openrouter
  max_context: 128000
  cost_per_input_token: 2e-06
  cost_per_output_token: 6e-06
